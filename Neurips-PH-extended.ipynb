{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NeurIPS Meetup Workshop\n",
    "\n",
    "#### Title: [\"Datasist‚Äù: A Python Library for Easy Data Analysis, Visualization and Modelling.](https://arxiv.org/abs/1911.03655)\n",
    "\n",
    "<img src=\"dlogo.jpeg\" width=\"300px\" height=\"250px\" alt='datasist logo'/>\n",
    "\n",
    "### Presented by: [Rising Odegua](https://twitter.com/risin_developer)\n",
    "\n",
    "#### Email: risingodegua@gmail.com\n",
    "#### Phone: +234-8140299072\n",
    "\n",
    "__________________________________\n",
    "#### What is Datasist?\n",
    "Datasist makes data analysis, visualization, cleaning preparation, and even modeling super easy for you during prototyping. \n",
    "\n",
    "Because let's face it I wouldn't want to do this...(Please look at the code block below)\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('some_csv_file.csv)\n",
    "                 \n",
    "missing_percent = (data.isna().sum() / data.shape[0]) * 100\n",
    "cols_2_drop = missing_percent[missing_percent.values >= 80].index\n",
    "#Drop missing values\n",
    "df = data.drop(cols_2_drop, axis=1)\n",
    "\n",
    "```\n",
    "\n",
    "...just because I want to drop columns with missing percentage greater than or equal to 80, when I can simply do this (Please look at the beauty below)\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import datasist as ds\n",
    "\n",
    "data = pd.read_csv('some_csv_file.csv)\n",
    "df = ds.drop_missing(data=data, percent=80)\n",
    "\n",
    "```\n",
    "\n",
    "*smiles, I know right, it's lazy, but efficient. \n",
    "\n",
    "The goal of datasist is to abstract repetitive and mundane codes, functions and techniques we use into simple, short functions and methods that can be called. Datasist was born out of sheer laziness, because let's face it unless you're a 10x data scientists, we all hate typing long, boring and mundane chunks of code to do the same thing over and over again. \n",
    "\n",
    "The design of datasist is currently centered around 6 modules, namely:\n",
    "\n",
    "1. structdata\n",
    "2. Feature Engineering \n",
    "3. timeseries\n",
    "4. visualization\n",
    "5. model\n",
    "\n",
    "This is subject to change in future versions as we are currently working on support for many other areas in the field. \n",
    "\n",
    "The aim of this post is to introduce you to some of the most important features in each of these modules and how you can start using it in your projects. \n",
    "For this post to be short and concise, I have splitted it into two parts. \n",
    "__________________________________\n",
    "\n",
    "What you will learn in this workshop:\n",
    "\n",
    "* Working with the datasist structdata module.\n",
    "* Feature engineering with datasist.\n",
    "* Easy visualization with datasist.\n",
    "* Testing and comparing machine learning models with datasist.\n",
    "\n",
    "#### Installing datasist\n",
    "\n",
    "```python\n",
    "    pip install datasist\n",
    "```\n",
    "Remember to use the exclamation symbol of you're running the command from a Jupyter notebook. \n",
    "\n",
    "```python\n",
    "    !pip install datasist\n",
    "```\n",
    "\n",
    "Next, you need to get a dataset to play with, you can use any dataset, but for consistency, you can download the dataset I used for this workshop [here](https://zindi.africa/competitions/data-science-nigeria-2019-challenge-1-insurance-prediction/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datasist as ds  #import datasist library\n",
    "import numpy as np\n",
    "\n",
    "train_df = pd.read_csv('train_data.csv')\n",
    "test_df = pd.read_csv('test_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with the structdata module\n",
    "\n",
    "The structdata module contains numerous functions for working with structured data mostly in the Pandas DataFrame format. That is, you can use the functions in this module to easily manipulate and analysis DataFrames. We highlight some of the functions available.\n",
    "\n",
    "1. describe function: We all know the describe function in Pandas, well we decided to extend it to support full description of a dataset at a glance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.structdata.describe(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the result, you can have a full description and properly understand some of the important features of your dataset at a glance, and in one line. \n",
    "\n",
    "2. check_train_test_set: This function is used to check the sampling strategy of two dataset. This is important because if two dataset are not from the same distrbution, then the feature extraction will be different as we can not use calculations on the first set-say train- on the next dataset-say test. \n",
    "\n",
    "To use this, you pass in the datasets (train_df and test_df), a common index (customer_id) and finally any feature or column available in both dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.structdata.check_train_test_set(train_df, test_df, index='Customer Id', col='Building Dimension')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. display_missing: You can check for the missing values in your dataset and display the result in the well formated DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.structdata.display_missing(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. get_cat_feats and get_num_feats: Just like their names, you can retrieve categorical and numerical features respectively as a list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_feats = ds.structdata.get_cat_feats(train_df)\n",
    "cat_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_feats = ds.structdata.get_num_feats(train_df)\n",
    "num_feats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. get_unique_counts: Ever wanted to get the unique classes in your categorical features before you decide what encoding scheme to use? well, you can use the get_unique_count function to easily that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.structdata.get_unique_counts(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. join_train_and_test: Most of the time when prototyping, you may want to concatenate both train and test set, and then apply some transformations to them. You can use the join_train_and_test function for that. It returns a concatenated dataset and the size of the train and test data for splitting in the future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data, ntrain, ntest = ds.structdata.join_train_and_test(train_df, test_df)\n",
    "print(\"New size of combined data {}\".format(all_data.shape))\n",
    "print(\"Old size of train data: {}\".format(ntrain))\n",
    "print(\"Old size of test data: {}\".format(ntest))\n",
    "\n",
    "#later splitting after transformations\n",
    "train = all_data[:ntrain]\n",
    "test = all_data[ntrain:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Those are some of the popular functions in the structdata module of datasist, to see other functions and to learn more about the parameters you can tweak, check the [API documentation here](). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature engineering with datasist.\n",
    "\n",
    "Feature engineering is the creation, manipu......\n",
    "\n",
    "Some of the functions available in the feature_engineering module of datasist are:\n",
    "\n",
    "NOTE: Functions in the feature_engineering module always returns a new and transformed DataFrame. This means, it always expects that you assign the result to a variable as nothing happens inplace. \n",
    "\n",
    "1. drop_missing: This function drops columns/features with a specified percentage of missing values. Assuming I have a set features with say 90 percent mssing values, I would want to drop these particular columns. I can do this with the drop_missing function. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first let's see the percentage of missing values\n",
    "ds.structdata.display_missing(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just for demonstration, we'll drop the column with 7.1 percent missing values.\n",
    "Note: You do not want to be dropping a column/feature with so little missing values. What you should do is ti fill it, but we do this here, just for demonstration purposes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train_df = ds.feature_engineering.drop_missing(train_df, percent=7.0)\n",
    "ds.structdata.display_missing(new_train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. drop_redundant: This function is used to remove features with low or no variance. That is features that contain the same class all through. We show a simple example using an artificial dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'a': [1,1,1,1,1,1,1],\n",
    "                  'b': [2,3,4,5,6,7,8]})\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now looking at the artificial dataset above, we see that column __a__ and __b__ are redundant, that is they have the same class all through. We can drop these columns automatically by just passing in the dataset to the drop_redundant function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = ds.feature_engineering.drop_redundant(df)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. convert_dtypes: This function takes a DataFrame and automatically type cast features that are not represented in their right types.Let's see an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'Name':['Tom', 'nick', 'jack'],\n",
    "        'Age':['20', '21', '19'], \n",
    "        'Date of Birth': ['1999-11-17','20 Sept 1998','Wed Sep 19 14:55:02 2000']}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The features Age and Date of Birth are suppose to be integer and Datetime respectively, by passing this DataFrame to the convert_dtype function, this can be automatically fixed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = ds.feature_engineering.convert_dtype(df)\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. fill_missing_cats: As the name implies, this function takes a DataFrame, and automatically detect categorical columns with missing values. It fills them using the mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.structdata.display_missing(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the dataset, we have two categorical features with missing values, these are Garden and Geo_Code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = ds.feature_engineering.fill_missing_cats(train_df)\n",
    "ds.structdata.display_missing(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. fill_missing_nums: This is similar to the fill_missing_cats, except it works on numerical features and you can specify a filling strategy (mean, mode or median). \n",
    "\n",
    "From the dataset, we have two numerica features with missing values, these are Building Dimension and Date_of_Occupancy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = ds.feature_engineering.fill_missing_num(train_df)\n",
    "ds.structdata.display_missing(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. log_transform: This function can help you log transform a set of features, It can also display before and after plot with the level of skewness to help you decide if log transforming is what you want.\n",
    "\n",
    "After visualization of some of the data set which we will study next, we found out that Building Dimension and Date_of_Occupancy are skewed. Let's use the log_transform function on them.\n",
    "\n",
    "Note: Make sure your columns do not contain missing values, else it will throw and error. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = ds.feature_engineering.fill_missing_num(df)\n",
    "df = ds.feature_engineering.log_transform(df, columns=['Building Dimension'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To work with features like latitude and longitude, datasist has dedicated functions like bearing, manhattan_distance, get_location_center etc, available in the feature_engineering module. I'll leave that for you to explore. To see these and other functions, you can check the API documentation here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WORKING WITH TIME BASED FEATURES\n",
    "\n",
    "Finally in this part, I'll talk about the timeseries module in datasist. The timeseries module contains functions for working with date time features. It can help you extract new features from Date features and help you visualize Date Features.\n",
    "\n",
    "1. extract_dates: This function can extract specified features like day of the week, day of the year, hour, min and second of the day from a specified date feature. \n",
    "To demonstrate this, let's use a dataset that contains Date feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train = pd.read_csv(\"sendy_train.csv\")\n",
    "new_train.head(3).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is logistic dataset, and contains lot's of time features which we can analyse. Let's demonstrate how easy it is to extract information from __Placement - Time__, __Arrival at Destination - Time__ features using the extract_dates function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['Placement - Time', 'Arrival at Destination - Time']\n",
    "df = ds.timeseries.extract_dates(new_train, date_cols=cols)\n",
    "df.head(3).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: You can specify the features to return by changing the subset parameter. For instance, we could specify that we only want day of the week and hour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['Placement - Time', 'Arrival at Destination - Time']\n",
    "df = ds.timeseries.extract_dates(new_train, date_cols=cols, subset=['dow', 'hr'])\n",
    "df.head(3).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. timeplot: The timeplot function can help you visualize a set feature against a particulae time feature. This can help you identify trends and patterns in these features. To use this function, you can pass a set of numerical cols, and then specify the Date feature you want to plot against."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols = ['Time from Pickup to Arrival', 'Destination Long', 'Pickup Long','Platform Type', 'Temperature']\n",
    "ds.timeseries.timeplot(new_train, num_cols=num_cols,\n",
    "                       time_col='Placement - Time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols = ['Time from Pickup to Arrival', 'Destination Long', 'Pickup Long','Platform Type', 'Temperature']\n",
    "ds.timeseries.timeplot(new_train, num_cols=num_cols,\n",
    "                       time_col='Pickup - Time')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "______________________________________________\n",
    "### Easy visualization using datasist.\n",
    "\n",
    "The visualization module is one of the strong areas of datasist. There are lots of functions that you can use to create aesthetic and colorful plots with minimal codes. In this post, I'll highlight some of the most import functions available in this module.\n",
    "\n",
    "Note: All functions in the visualization module works at data scale not feature scale. This means, you can pass in the full dataset and it visualization every feature out of the box. You can also specify the features you want to plot.\n",
    "\n",
    "#### VISUALIZATION FOR CATEGORICAL FEATURES\n",
    "\n",
    "Visualization for numerical features include plots like scaterplot, histogram, kde plots etc. We can use the functions available in datasist to easily do this at data wide level. \n",
    "\n",
    "1. boxplot: This function makes a box plot of all numerical features against a specified categorical target column. \n",
    "\n",
    "Note: You can save a plot as a png file in the current folder by setting the save_fig parameter to True in any of the visualization function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.visualizations.boxplot(train_df, target='Claim')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. catbox: The catbox feature is used to make a side by side bar plot of all categorical features in a dataset against a specified categorical target. This can help in identifying causation and patterns and also identifying features that seperates the specified target properly.\n",
    "\n",
    "Note: catbox would only plot categorical feature with a limited number of unique classes.Also, the target must be a categorical feature with a limited number of unique classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.visualizations.catbox(train_df, target='Claim')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. countplot: The countplot simply makes a barplot of all categorical feature to show their class count. \n",
    "\n",
    "Note: You can specify specific features to plot else, it is automatically inferred. You can also specify a seperate by feature. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.visualizations.countplot(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.visualizations.countplot(train_df, separate_by='Claim')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### VISUALIZATION FOR NUMERICAL FEATURES\n",
    "\n",
    "Visualization for numerical features include plots like scaterplot, histogram, kde plots etc. We can use the functions available in datasist to easily do this at data wide level. \n",
    "\n",
    "1. histogram: This function makes an histogran plot of all numerical features in a dataset. This Helps to show distribution of the features.\n",
    "\n",
    "Note: To use this, the specified features to plot must not contain missing values, else it would throw an error.\n",
    "\n",
    "In our example below, the features Building Dimension and Date_of_Occupancy both contain missing values. We could decide to fill this before plotting or we could pass in a list with these features removed.\n",
    "\n",
    "I'll go with the first option, that is filling the missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = ds.feature_engineering.fill_missing_num(train_df)\n",
    "ds.visualizations.histogram(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. scatterplot: This function makes a scatter plot of all numerical features in a dataset against a numerical target. It helps to show the correlation between features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats = ['Insured_Period',\n",
    "         'Residential',\n",
    "         'Building Dimension',\n",
    "         'Building_Type',\n",
    "         'Date_of_Occupancy']\n",
    "\n",
    "ds.visualizations.scatterplot(train_df,num_features=feats, target='Building Dimension')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. plot_missing: As the name implies, this function can be used to visualize the missing values in a dataset. White cells indicate missing and dark cells indicate full. The color range at the right hand corner shows intensity values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.visualizations.plot_missing(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____________________________________________________________________\n",
    "#### Testing and comparing machine learning models with datasist\n",
    "\n",
    "The __model__ module contains functions and methods for testin and comparing machine learning models. Current version of datasist only supports scikit-learn models. Tensorflow and Pytorch models will be supported soon. \n",
    "I'll highlight some of the important functions in this model, and also show you how you can use the metrics visualization functions in the visualization module along side. \n",
    "\n",
    "To demostrate these functions, we'll use a dataset from the Data Science Nigeria, 2019 BootCamp available here. The task is to predict insurance claim (1=Claim, 0=No Claim) from building observations. \n",
    "We'll do some basic data preprocessing and prepare the data for modeling.\n",
    "Note: The goal of this analysis is to demonstrate how to use the model module, so we would not be doing any heavy feature engineering. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', 400)\n",
    "train = pd.read_csv('train_data.csv')\n",
    "test = pd.read_csv('test_data.csv')\n",
    "vardef = pd.read_csv(\"variabledef.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vardef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop the id column\n",
    "train.drop(columns='Customer Id', axis=1, inplace=True)\n",
    "test.drop(columns='Customer Id', axis=1, inplace=True)\n",
    "\n",
    "#fill missing values\n",
    "train = ds.feature_engineering.fill_missing_cats(train)\n",
    "train = ds.feature_engineering.fill_missing_num(train, method='mean')\n",
    "\n",
    "test = ds.feature_engineering.fill_missing_cats(test)\n",
    "test = ds.feature_engineering.fill_missing_num(test, method='mean')\n",
    "\n",
    "ds.structdata.display_missing(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have properly filled dataset, next we'll encode all categorical features using either label encoding, or one hot encoding depending on the number of unique classes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check the unique classes in each categorical feature\n",
    "ds.structdata.class_count(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will label encode Geo_Code, since the unique classes is large, and one-hot-encode the rest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import category_encoders as ce\n",
    "\n",
    "# drop target column\n",
    "target = train['Claim'].values\n",
    "train.drop(columns='Claim', axis=1, inplace=True)\n",
    "\n",
    "enc = ce.OrdinalEncoder(cols=['Geo_Code'])\n",
    "enc.fit(train)\n",
    "train_enc = enc.transform(train)\n",
    "test_enc = enc.transform(test)\n",
    "\n",
    "\n",
    "#one-hot-encode the rest categorical features\n",
    "hot_enc = ce.OneHotEncoder()\n",
    "hot_enc.fit(train_enc)\n",
    "train_enc = hot_enc.transform(train_enc)\n",
    "test_enc = hot_enc.transform(test_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_enc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape of train data after encoding: {}\".format(train_enc.shape))\n",
    "print(\"Shape of test data after encoding: {}\".format(test_enc.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. compare_model: This model takes as argument multiple machine learning models and returns a plot of the metric. This can be used to pick a base model for and also to compare models side by side. The compare model returns a tuple of the trained models and their score in case you want to make predictions with the best model.\n",
    "\n",
    "Now let's compare some classification model. We'll compare RandomForest, LightGBM and XGBoost models. \n",
    "\n",
    "Note: We won't be performing any advance hyperparameter tuning in this session, as the goal is to show you how to use the functions and not extensive hyperparameter tunings. \n",
    "\n",
    "Also, you will have to install lightgbm and xgboost before you can try this part. Alternatively, you can use the default models in scikit-learn. To install lightgbm go here and to install xgboost, go here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(train_enc, target, test_size=0.3, random_state=1)\n",
    "rf_classifier = RandomForestClassifier(n_estimators=20, max_depth=4)\n",
    "lgb_classifier = lgb.LGBMClassifier(n_estimators=20, max_depth=4)\n",
    "xgb_classifier = xgb.XGBClassifier(n_estimators=20, max_depth=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = [rf_classifier, lgb_classifier, xgb_classifier]\n",
    "models, scores = ds.model.compare_model(models_list=classifiers, x_train=Xtrain, y_train=ytrain, scoring_metric='accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this sample analysis, the LGBMClassifier is currently the best model. We can make predictions using this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = models[1].predict(Xtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. get_classification_report: We can get a detailed metric report for a classification task using the get_classification_report function. This accepts as argument the predicted class and the truth values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.model.get_classification_report(pred, ytest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. plot_feature_importance: This function will make a bar plot of the most important features of a trained model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models[1]  #get a model from the list of returned models\n",
    "features = train_enc.columns  #get the feature names from the processed data\n",
    "\n",
    "ds.model.plot_feature_importance(model, features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: We demonstrated the examples using a classification task. You can also apply the same functions to your regression problems. Other functions available in the model model is train_classifier and make_submission_file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____________________________________________________________\n",
    "[LINK](https://github.com/risenW/datasist) TO DATASIST REPO ON GITHUB\n",
    "\n",
    "[LINK](https://risenw.github.io/datasist/index.html) TO API DOCUMENTATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "THANK YOU FOR LISTENING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
